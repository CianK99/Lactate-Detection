{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f872c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, PredictionErrorDisplay, make_scorer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200f6d5",
   "metadata": {},
   "source": [
    "#### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7908d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"DataFrame_PathLen.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_columns = data.columns[:2]\n",
    "end_columns = data.columns[-8:]\n",
    "data = data.drop(columns=end_columns)\n",
    "reversed_columns = list(data.columns[2:][::-1])\n",
    "data = pd.concat([data[start_columns], data[reversed_columns]], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13902b72",
   "metadata": {},
   "source": [
    "#### Organising it unto the subsets based on pathlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfe7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data[data['PathLength'] == 2]\n",
    "data_5 = data[data['PathLength'] == 5]\n",
    "data_10 = data[data['PathLength'] == 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b47e3",
   "metadata": {},
   "source": [
    "#### Visualisation of our Average intensity by path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077483b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean of the wavelength feature columns for each DataFrame\n",
    "avg_spectrum_ovrll = data.iloc[:, 2:].mean()\n",
    "avg_spectrum_2 = data_2.iloc[:, 2:].mean()\n",
    "avg_spectrum_5 = data_5.iloc[:, 2:].mean()\n",
    "avg_spectrum_10 = data_10.iloc[:, 2:].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "avg_spectrum_ovrll.plot(label='Overall Data')\n",
    "avg_spectrum_2.plot(label='Data 2')\n",
    "avg_spectrum_5.plot(label='Data 5')\n",
    "avg_spectrum_10.plot(label='Data 10')\n",
    "\n",
    "plt.title('Average Response')\n",
    "plt.xlabel('Wavelength')\n",
    "plt.ylabel('Average Intensity')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ad8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_vs_predicted(y_true, y_pred, column_name):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    # Creating a scatter plot of actual vs. predicted values with partial transparency\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.xlabel(f'Actual {column_name}')\n",
    "    plt.ylabel(f'Predicted {column_name}')\n",
    "    plt.title(f'Observed vs. Predicted for {column_name}')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Determining the maximum value found in either the actual or predicted data\n",
    "    max_value = max(y_true.max(), y_pred.max())\n",
    "    # Drawing a line from (0,0) to (max_value, max_value) to show the line of perfect prediction\n",
    "    plt.plot([0, max_value], [0, max_value], linestyle='--', color='red')\n",
    "    \n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    plt.savefig('plots/Observed_vs_predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc82eb8",
   "metadata": {},
   "source": [
    "Cross-validated grid search to hyper-parameterise number of components in PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50947965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_pls(data, k=10, n_components_range=None):\n",
    "    X = data[data.columns[2:]]\n",
    "    y = data['Concentration']\n",
    "\n",
    "    # Define range for n_components if not provided\n",
    "    if n_components_range is None:\n",
    "        n_components_range = list(range(1, min(X.shape[1], 15) + 1))  # 1 to 15 or number of features\n",
    "\n",
    "    # Define PLS model and parameter grid\n",
    "    pls = PLSRegression()\n",
    "    param_grid = {'n_components': n_components_range}\n",
    "\n",
    "    # Define scorer and grid search\n",
    "    scorer = make_scorer(r2_score)\n",
    "    grid_search = GridSearchCV(pls, param_grid, cv=k, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "    # Fit grid search\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Best R2 Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"All Path Lengths\": undersampled_data,\n",
    "    \"Path Length 2\": undersampled_data_2,\n",
    "    \"Path Length 5\": undersampled_data_5,\n",
    "    \"Path Length 10\": undersampled_data_10\n",
    "}\n",
    "for name,df in dataframes.items():\n",
    "    print(str(name))\n",
    "    best_params = grid_search_pls(df)\n",
    "    optimal_components = best_params['n_components']\n",
    "\n",
    "    # Use this optimal number of components in your PLS model\n",
    "    pls(df, k=10, graph=False, n_components=optimal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90830740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls(data, n_components=14, k=10, graph=False):\n",
    "    X = data[data.columns[2:]]\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=4)\n",
    "    r2_scores = {}\n",
    "    rmse_scores = {}\n",
    "    mae_scores = {}\n",
    "    column = 'Concentration'\n",
    "\n",
    "    y = data[column]\n",
    "    \n",
    "    # Initialise empty lists to collect scores for the target column\n",
    "    r2_scores[column] = []\n",
    "    rmse_scores[column] = []\n",
    "    mae_scores[column] = []\n",
    "\n",
    "    # Loop through each fold in k-fold cross-validation\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Standardise features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Fit PLS model\n",
    "        model = PLSRegression(n_components=n_components)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test_scaled).ravel()\n",
    "\n",
    "        # Metrics for this fold\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_scores[column].append(r2)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rmse_scores[column].append(rmse)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores[column].append(mae)\n",
    "\n",
    "    # Plot last fold results if graph is True\n",
    "    if graph:\n",
    "        plot_observed_vs_predicted(y_test, y_pred, column)\n",
    "\n",
    "    # Compute and display average metrics\n",
    "    r2_scores[column] = sum(r2_scores[column]) / len(r2_scores[column])\n",
    "    rmse_scores[column] = sum(rmse_scores[column]) / len(rmse_scores[column])\n",
    "    mae_scores[column] = sum(mae_scores[column]) / len(mae_scores[column])\n",
    "\n",
    "    print(\"Average Evaluation Metrics:\")\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"R2 Score: {r2_scores[column]:.4f}\")\n",
    "    print(f\"RMSE: {rmse_scores[column]:.4f}\")\n",
    "    print(f\"MAE: {mae_scores[column]:.4f}\")\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(data, k=10, graph=False):\n",
    "    X = data[data.columns[2:]]\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=4)\n",
    "    r2_scores = {}\n",
    "    rmse_scores = {}\n",
    "    mae_scores = {}\n",
    "    column = 'Concentration'\n",
    "\n",
    "    y = data[column]\n",
    "    \n",
    "    # Initialise empty lists to collect scores for the target column\n",
    "    r2_scores[column] = []\n",
    "    rmse_scores[column] = []\n",
    "    mae_scores[column] = []\n",
    "\n",
    "    \n",
    "    # Iterating over each fold specified by KFold.\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Standardise features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Cleaner output\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        \n",
    "        # Fit LASSO model\n",
    "        model = Lasso(random_state=4)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Metrics for this fold\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_scores[column].append(r2)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rmse_scores[column].append(rmse)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores[column].append(mae)\n",
    "\n",
    "    # Plot last fold results if graph is True\n",
    "    if graph == True:\n",
    "        plot_observed_vs_predicted(y_test, y_pred, column)\n",
    "\n",
    "        \n",
    "    # Compute and display average metrics\n",
    "    r2_scores[column] = sum(r2_scores[column]) / len(r2_scores[column])\n",
    "    rmse_scores[column] = sum(rmse_scores[column]) / len(rmse_scores[column])\n",
    "    mae_scores[column] = sum(mae_scores[column]) / len(mae_scores[column])\n",
    "\n",
    "    print(\"Average Evaluation Metrics:\")\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"R2 Score: {r2_scores[column]}\")\n",
    "    print(f\"RMSE: {rmse_scores[column]}\")\n",
    "    print(f\"MAE: {mae_scores[column]}\")\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_forest(data, k=10, graph=False, evaluate_train_err=False):\n",
    "    X = data[data.columns[2:]]\n",
    "    y = data['Concentration']\n",
    "    column = 'Concentration'\n",
    "    \n",
    "    # Configuring the k-fold cross-validation setup with shuffling.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=4) \n",
    "\n",
    "    \n",
    "    # Initialising dictionaries to store performance metrics for testing and optionally for training.\n",
    "    test_scores = {'r2': [], 'rmse': [], 'mae': []}\n",
    "    train_scores = {'r2': [], 'rmse': [], 'mae': []} if evaluate_train_err else None\n",
    "\n",
    "    # Iterating over each fold specified by KFold.\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Standardising the features to mean zero and unit variance.\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Training the Random Forest model.\n",
    "        model = RandomForestRegressor(random_state=4, n_jobs=-1)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Making predictions and calculating test metrics.\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        test_scores['r2'].append(r2_score(y_test, y_test_pred))\n",
    "        test_scores['rmse'].append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "        test_scores['mae'].append(mean_absolute_error(y_test, y_test_pred))\n",
    "        \n",
    "        # Optionally calculating training metrics if required.\n",
    "        if evaluate_train_err:\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            train_scores['r2'].append(r2_score(y_train, y_train_pred))\n",
    "            train_scores['rmse'].append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "            train_scores['mae'].append(mean_absolute_error(y_train, y_train_pred))\n",
    "\n",
    "    # If training error evaluation is enabled, visualising the comparison between train and test metrics.\n",
    "    if evaluate_train_err:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        metric_titles = ['R2 Scores', 'RMSE', 'MAE']\n",
    "        for i, metric in enumerate(['r2', 'rmse', 'mae']):\n",
    "            box = axs[i].boxplot([train_scores[metric], test_scores[metric]], labels=['Train', 'Test'], patch_artist=True, medianprops={'color': 'black', 'linewidth': 2})\n",
    "\n",
    "            for patch, color in zip(box['boxes'], ['skyblue', 'salmon']):\n",
    "                patch.set_facecolor(color)\n",
    "\n",
    "            axs[i].set_title(metric_titles[i])\n",
    "        plt.suptitle('Comparison of Train and Test Metrics Across Folds')\n",
    "        plt.show()\n",
    "\n",
    "    # Optionally plotting the prediction error for the last fold if graphing is enabled.\n",
    "    if graph:\n",
    "        plot_observed_vs_predicted(y_test, y_test_pred, column)\n",
    "\n",
    "    # Printing the average test metrics.\n",
    "    print(f\"Average Test Metrics:\")\n",
    "    for metric in test_scores:\n",
    "        print(f\"{metric.upper()}: {np.mean(test_scores[metric])}\")\n",
    "\n",
    "    # Optionally printing the average training metrics.\n",
    "    if evaluate_train_err:\n",
    "        print(f\"\\nAverage Train Metrics:\")\n",
    "        for metric in train_scores:\n",
    "            print(f\"{metric.upper()}: {np.mean(train_scores[metric])}\")\n",
    "\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a138aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    #\"All Path Lengths\": data,\n",
    "    \"Path Length 2\": data_2,\n",
    "    \"Path Length 5\": data_5,\n",
    "    \"Path Length 10\": data_10\n",
    "}\n",
    "\n",
    "# Looping through the dictionary items.\n",
    "for name,df in dataframes.items():\n",
    "    print('Dataset: ' + name)\n",
    "    print('PLS:')\n",
    "    # 15 is the optimal for Path Length 2, the best performing for PLS\n",
    "    pls(df, n_components=15)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Dataset: ' + name)\n",
    "    print('Random Forest:')\n",
    "    rand_forest(df)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Dataset: ' + name)\n",
    "    print('Lasso:')\n",
    "    lasso(df)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be7bf8",
   "metadata": {},
   "source": [
    "## Undersampled data\n",
    "This is the methodology to ensure an even distrubution of sdamples across path lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25324c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sample rows from the dataset where Path Length is 2 and Concentration is 0\n",
    "remove_from_2_0 = data.loc[(data['PathLength'] == 2) & (data['Concentration'] == 0)].sample(24, random_state=4)\n",
    "\n",
    "# Filter and sample rows from the dataset where Path Length is 5 and Concentration is 0\n",
    "remove_from_5_0 = data.loc[(data['PathLength'] == 5) & (data['Concentration'] == 0)].sample(7, random_state=4)\n",
    "\n",
    "# Filter and sample rows from the dataset where Path Length is 2 and Concentration is 1.3\n",
    "remove_from_2_1 = data.loc[(data['PathLength'] == 2) & (data['Concentration'] == 1.3)].sample(72, random_state=4)\n",
    "\n",
    "# Filter and sample rows from the dataset where Path Length is 10 and Concentration is 1.3\n",
    "remove_from_10_1 = data.loc[(data['PathLength'] == 10) & (data['Concentration'] == 1.3)].sample(45, random_state=4)\n",
    "\n",
    "removals = [remove_from_2_0,remove_from_5_0,remove_from_2_1,remove_from_10_1]\n",
    "removals_df = pd.concat(removals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55503c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows specified in removals_df from the original dataset to create an undersampled dataset\n",
    "undersampled_data = data.drop(removals_df.index)\n",
    "\n",
    "# Filtering the undersampled dataset to get entries by Path Length\n",
    "undersampled_data_2 = undersampled_data[undersampled_data['PathLength'] == 2]\n",
    "undersampled_data_5 = undersampled_data[undersampled_data['PathLength'] == 5]\n",
    "undersampled_data_10 = undersampled_data[undersampled_data['PathLength'] == 10]\n",
    "\n",
    "# Printing the count of entries for each Path Length after undersampling\n",
    "print(len(undersampled_data_2),len(undersampled_data_2),len(undersampled_data_10))\n",
    "# Printing the count of entries for each Path Length before any removals for comparison\n",
    "print(len(data_2),len(data_5),len(data_10))\n",
    "\n",
    "print(len(undersampled_data_2) + len(undersampled_data_5) + len(undersampled_data_10))\n",
    "print(len(data_2) + len(data_5) + len(data_10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b4f5d",
   "metadata": {},
   "source": [
    "Running the models on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"All Path Lengths\": undersampled_data,\n",
    "    \"Path Length 2\": undersampled_data_2,\n",
    "    \"Path Length 5\": undersampled_data_5,\n",
    "    \"Path Length 10\": undersampled_data_10\n",
    "}\n",
    "\n",
    "for name,df in dataframes.items():\n",
    "    \n",
    "    print('Dataset: ' + name)\n",
    "    print('Random Forest:')\n",
    "    rand_forest(df)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Dataset: ' + name)\n",
    "    print('Lasso:')\n",
    "    lasso(df)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Dataset: ' + name)\n",
    "    print('PLS:')\n",
    "    # 15 is the optimal for Path Length 2, the best performing for PLS\n",
    "    pls(df, n_components=15)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb0c1e",
   "metadata": {},
   "source": [
    "| **Dataset**         | **Model**          | **R² Score**      | **RMSE**       | **MAE**       |\n",
    "|----------------------|--------------------|-------------------|----------------|---------------|\n",
    "| **All Path Lengths** | **Random Forest** | **0.9778**        | **76.38**      | **18.84**     |\n",
    "|                      | LASSO             | 0.8746            | 196.88         | 147.47        |\n",
    "|                      | PLS               | 0.8730            | 198.16         | 148.21        |\n",
    "| **Path Length 2**    | Random Forest     | 0.9697            | 65.05          | 14.23         |\n",
    "|                      | LASSO             | 0.9988            | 18.55          | 13.43         |\n",
    "|                      | **PLS**           | **0.9993**        | **14.05**      | **10.81**     |\n",
    "| **Path Length 5**    | **Random Forest** | **0.9977**        | **18.96**      | **4.87**      |\n",
    "|                      | LASSO             | 0.9948            | 35.12          | 25.51         |\n",
    "|                      | PLS               | 0.9938            | 34.47          | 23.00         |\n",
    "| **Path Length 10**   | **Random Forest** | **0.9986**        | **15.93**      | **3.97**      |\n",
    "|                      | LASSO             | 0.9958            | 35.91          | 28.07         |\n",
    "|                      | PLS               | 0.9951            | 38.65          | 30.99         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6152a",
   "metadata": {},
   "source": [
    "### This is the feature selection Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7969a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_feature_removal(df, final_feature_count=1, filename='feature_removal_metrics.png'):\n",
    "    X = df.iloc[:, 2:]\n",
    "    y = df['Concentration']\n",
    "    \n",
    "    # Dictionary to hold metrics for each iteration\n",
    "    metrics = {\n",
    "        'Train R2': [], 'Train MAE': [], 'Train RMSE': [],\n",
    "        'Test R2': [], 'Test MAE': [], 'Test RMSE': [],\n",
    "        'Features': []\n",
    "    }\n",
    "    # List of all features in the dataset\n",
    "    features = X.columns.tolist()\n",
    "    \n",
    "    # Parameters for feature reduction\n",
    "    initial_reduction_factor = 0.2 # Initial percentage of features to remove each iteration\n",
    "    refinement_threshold = 20 # When the number of features drops below this, reduce one at a time\n",
    "\n",
    "    # KFold setup for cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=4)\n",
    "    \n",
    "    # Variables to track the best model\n",
    "    best_r2 = -np.inf\n",
    "    best_features = None\n",
    "    iteration = 0\n",
    "    \n",
    "    # Iteratively reduce features until the count reaches final_feature_count\n",
    "    while len(features) > final_feature_count:\n",
    "        # Lists to store metrics for each fold\n",
    "        train_r2_scores = []\n",
    "        train_mae_scores = []\n",
    "        train_rmse_scores = []\n",
    "        test_r2_scores = []\n",
    "        test_mae_scores = []\n",
    "        test_rmse_scores = []\n",
    "\n",
    "        # Cross-validation loop\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[features].iloc[train_index], X[features].iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "            # Training the Random Forest model\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=4, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicting and computing metrics for both train and test sets\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            train_r2_scores.append(r2_score(y_train, y_train_pred))\n",
    "            train_mae_scores.append(mean_absolute_error(y_train, y_train_pred))\n",
    "            train_rmse_scores.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "\n",
    "            test_r2_scores.append(r2_score(y_test, y_test_pred))\n",
    "            test_mae_scores.append(mean_absolute_error(y_test, y_test_pred))\n",
    "            test_rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "\n",
    "        # Calculating average metrics for the current set of features\n",
    "        average_test_r2 = np.mean(test_r2_scores)\n",
    "        metrics['Train R2'].append(np.mean(train_r2_scores))\n",
    "        metrics['Train MAE'].append(np.mean(train_mae_scores))\n",
    "        metrics['Train RMSE'].append(np.mean(train_rmse_scores))\n",
    "        metrics['Test R2'].append(average_test_r2)\n",
    "        metrics['Test MAE'].append(np.mean(test_mae_scores))\n",
    "        metrics['Test RMSE'].append(np.mean(test_rmse_scores))\n",
    "        metrics['Features'].append(len(features))\n",
    "        \n",
    "        # Update the best model if the current one is better\n",
    "        if average_test_r2 > best_r2:\n",
    "            best_r2 = average_test_r2\n",
    "            best_features = features.copy()\n",
    "\n",
    "        # Refitting to get feature importances\n",
    "        model.fit(X[features], y)\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_dict = {feature: imp for feature, imp in zip(features, importances)}\n",
    "        sorted_features = sorted(feature_importance_dict, key=feature_importance_dict.get)\n",
    "        \n",
    "        # Determining how many features to remove\n",
    "        if len(features) > refinement_threshold:\n",
    "            features_to_remove_count = max(int(len(features) * initial_reduction_factor), 1)\n",
    "        else:\n",
    "            features_to_remove_count = 1\n",
    "        \n",
    "        # Updating the feature list by removing the least important features\n",
    "        features = [f for f in features if f not in sorted_features[:features_to_remove_count]]\n",
    "        iteration += 1\n",
    "    \n",
    "    # Plotting metrics across iterations\n",
    "    iterations = list(range(1, iteration + 1))\n",
    "    metric_types = ['R2', 'MAE', 'RMSE']\n",
    "    for i, metric in enumerate(metric_types):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(iterations, metrics[f'Train {metric}'], label=f'Train {metric}', marker='o', linestyle='-', color='blue')\n",
    "        ax.plot(iterations, metrics[f'Test {metric}'], label=f'Test {metric}', marker='s', linestyle='--', color='green')\n",
    "        ax.set_xticks(iterations)\n",
    "        ax.set_xticklabels(metrics['Features'], rotation=45)\n",
    "        ax.set_xlabel('Number of Features')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'{metric} Over Iterations')\n",
    "        ax.legend()\n",
    "        # Inverting x axis to show in decreasing order of feature count\n",
    "        ax.invert_xaxis()\n",
    "\n",
    "        # Saving each metric's plot separately\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if not os.path.exists('plots'):\n",
    "            os.makedirs('plots')\n",
    "            \n",
    "        plt.savefig(f'plots/{metric}_{filename}', dpi=300)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Printing the best performing features and corresponding R2 score\n",
    "    print(best_features, best_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"Overall Data\": undersampled_data,\n",
    "    \"Path Length 2\": undersampled_data_2,\n",
    "    \"Path Length 5\": undersampled_data_5,\n",
    "    \"Path Length 10\": undersampled_data_10\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print('Database: ' + name)\n",
    "    iterative_feature_removal(df, filename=name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_undersampled_2 = ['1391.64', '1394.97', '1398.31', '1401.68', '1408.45', '1411.86', '2135.89', '2143.74', '2151.65', '2159.62', '2167.65', '2175.73']\n",
    "top_features_undersampled_5 = ['1278.72', '1281.53', '1284.35', '1295.77', '1325.22', '1331.27', '1334.32', '1337.38', '1343.54', '1346.64', '1349.76', '1362.38', '1371.99', '1695.05']\n",
    "top_features_undersampled_10 = ['1121.34', '1340.45', '1343.54', '1346.64', '1362.38', '1502.83', '1510.61', '1514.54', '1518.48', '1522.45', '1530.44', '1534.47', '1542.58', '1546.68', '1550.79', '1554.92', '1567.46', '1571.69', '1575.94', '1580.21', '1588.82', '1601.91', '1606.33', '1610.76', '1615.23', '1619.71']\n",
    "\n",
    "top_features_undersampled_2 = [float(col) for col in top_features_undersampled_2]\n",
    "top_features_undersampled_5 = [float(col) for col in top_features_undersampled_5]\n",
    "top_features_undersampled_10 = [float(col) for col in top_features_undersampled_10]\n",
    "\n",
    "print(len(top_features_undersampled_2),len(top_features_undersampled_5),len(top_features_undersampled_10))\n",
    "start_columns = ['Concentration', 'PathLength']\n",
    "features_list_undersampled_2 = start_columns + top_features_undersampled_2\n",
    "features_list_undersampled_5 = start_columns + top_features_undersampled_5\n",
    "features_list_undersampled_10 = start_columns + top_features_undersampled_10\n",
    "\n",
    "undersampled_data_2_top_features = undersampled_data_2[features_list_undersampled_2]\n",
    "undersampled_data_5_top_features = undersampled_data_5[features_list_undersampled_5]\n",
    "undersampled_data_10_top_features = undersampled_data_10[features_list_undersampled_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b428f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest(undersampled_data_2_top_features)\n",
    "rand_forest(undersampled_data_5_top_features)\n",
    "rand_forest(undersampled_data_10_top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27632e60",
   "metadata": {},
   "source": [
    "### Plot of the average intensity with the top features for each Path Length marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa05481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average spectrum from the undersampled data\n",
    "avg_spectrum = undersampled_data.iloc[:, 2:].mean()\n",
    "data_features = data.columns[2:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define x-axis limits based on the minimum and maximum wavelengths\n",
    "plt.xlim([min(data_features), max(data_features)])\n",
    "plt.plot(avg_spectrum, color='black', alpha=0.7, label='Average Intensity')\n",
    "\n",
    "# Define colours for each path length set for clarity in visual distinction\n",
    "colours = ['green','deepskyblue','orangered']\n",
    "\n",
    "# A list of lists containing top features (wavelengths) for each path length\n",
    "pathlengths = [top_features_undersampled_2,\n",
    "               top_features_undersampled_5,\n",
    "               top_features_undersampled_10]\n",
    "labels = ['Path Length 2', 'Path Length 5', 'Path Length 10']\n",
    "\n",
    "# Loop through each set of path lengths\n",
    "for idx, features in enumerate(pathlengths):\n",
    "    # Mark each feature with a vertical line\n",
    "    for wavelength in features:\n",
    "        plt.axvline(x=float(wavelength), color=colours[idx], linestyle='-', alpha=0.4, \n",
    "                    label=labels[idx] if features.index(wavelength) == 0 else \"\")\n",
    "\n",
    "plt.title('Optimum Wavelengths for Each Path Length Marked on a Plot of the Average Intensity by Wavelength Across All Path Lengths')\n",
    "plt.xlabel('Wavelength(nm)')\n",
    "plt.ylabel('Average Intensity(a.u.)')\n",
    "plt.legend()\n",
    "\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "    \n",
    "plt.savefig('plots/average_intensity_plot_all_pathlengths.png', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab87cd",
   "metadata": {},
   "source": [
    "## Errors at each concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_levels(y_true, y_pred, levels):\n",
    "    # Initialise a dictionary to store error metrics and errors for each level.\n",
    "    level_data = {level: {'mae': 0, 'rmse': 0, 'errors': {'mae': [], 'rmse': []}} for level in levels}\n",
    "    \n",
    "    # Loop through each specified level to compute metrics.\n",
    "    for level in levels:\n",
    "        # Identify the indices where the true values match the current level.\n",
    "        indices = (y_true == level)\n",
    "        y_true_level = y_true[indices]\n",
    "        y_pred_level = y_pred[indices]\n",
    "\n",
    "        # Calculate absolute errors and squared errors.\n",
    "        abs_errors = np.abs(y_true_level - y_pred_level)\n",
    "        sq_errors = (y_true_level - y_pred_level) ** 2\n",
    "\n",
    "        level_data[level]['errors']['mae'] = abs_errors\n",
    "        level_data[level]['errors']['rmse'] = sq_errors\n",
    "\n",
    "        # Compute the mean absolute error and root mean squared error.\n",
    "        level_data[level]['mae'] = np.mean(abs_errors)\n",
    "        level_data[level]['rmse'] = np.sqrt(np.mean(sq_errors))\n",
    "    \n",
    "    return level_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b12928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distributions(level_data):\n",
    "    # Determine the number of levels to set the number of subplots\n",
    "    num_levels = len(level_data)\n",
    "    # Creating a subplot grid: one row for each level and two columns for MAE and RMSE\n",
    "    fig, axs = plt.subplots(num_levels, 2, figsize=(12, num_levels * 3), squeeze=False)\n",
    "\n",
    "    # Looping through each level to plot its error distributions\n",
    "    for i, (level, data) in enumerate(level_data.items()):\n",
    "        # Extracting MAE and RMSE errors from the data\n",
    "        mae_errors = data['mae']\n",
    "        rmse_errors = data['rmse']\n",
    "\n",
    "        mae_bins = 30\n",
    "        rmse_bins = 30\n",
    "\n",
    "        # Plotting the histogram for MAE\n",
    "        axs[i, 0].hist(mae_errors, bins=mae_bins, color='skyblue', edgecolor='black')\n",
    "        axs[i, 0].set_title(f\"MAE Distribution at Level {level}\")\n",
    "        axs[i, 0].set_xlabel('MAE')\n",
    "        axs[i, 0].set_ylabel('Frequency')\n",
    "\n",
    "        # Plotting the histogram for RMSE\n",
    "        axs[i, 1].hist(rmse_errors, bins=rmse_bins, color='salmon', edgecolor='black')\n",
    "        axs[i, 1].set_title(f\"RMSE Distribution at Level {level}\")\n",
    "        axs[i, 1].set_xlabel('RMSE')\n",
    "        axs[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_forest_levels(data, k=10, importances=False):\n",
    "    X = data[data.columns[2:]]\n",
    "    y = data['Concentration']\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=4)    \n",
    "    test_scores = {'r2': [], 'rmse': [], 'mae': []}\n",
    "    levels = unique_levels = y.unique()\n",
    "    aggregated_errors = {level: {'mae': [], 'rmse': []} for level in levels}\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        model = RandomForestRegressor(random_state=4, n_jobs=-1)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        feature_importances += model.feature_importances_\n",
    "        \n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        test_scores['r2'].append(r2_score(y_test, y_test_pred))\n",
    "        test_scores['rmse'].append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "        test_scores['mae'].append(mean_absolute_error(y_test, y_test_pred))\n",
    "        \n",
    "        # Evaluate performance per level\n",
    "        fold_metrics_per_level = evaluate_levels(y_test, y_test_pred, levels)\n",
    "        for level in levels:\n",
    "            aggregated_errors[level]['mae'].extend(fold_metrics_per_level[level]['errors']['mae'])\n",
    "\n",
    "            rmse_per_fold = np.sqrt(np.mean(fold_metrics_per_level[level]['errors']['rmse']))\n",
    "            aggregated_errors[level]['rmse'].append(rmse_per_fold)\n",
    "        \n",
    "    # Plot error distributions for each level\n",
    "    plot_error_distributions(aggregated_errors)\n",
    "    \n",
    "    # Average feature importances over all folds\n",
    "    feature_importances /= k\n",
    "    \n",
    "    # Output average test metrics\n",
    "    print(f\"Average Test Metrics:\")\n",
    "    for metric in test_scores:\n",
    "        print(f\"{metric.upper()}: {np.mean(test_scores[metric])}\")\n",
    "\n",
    "    # Output metrics for each level \n",
    "    for level in levels:\n",
    "        if aggregated_errors[level]['mae'] and any(np.isfinite(aggregated_errors[level]['rmse'])):\n",
    "            print(f\"Metrics for Level {level}:\")\n",
    "            avg_mae = np.mean(aggregated_errors[level]['mae'])\n",
    "            finite_rmse = [value for value in aggregated_errors[level]['rmse'] if np.isfinite(value)]\n",
    "            avg_rmse = np.mean(finite_rmse) if finite_rmse else float('nan')\n",
    "\n",
    "            print(f\"  RMSE: {avg_rmse}\")\n",
    "            print(f\"  MAE: {avg_mae}\")\n",
    "        else:\n",
    "            print(f\"Metrics for Level {level}:\")\n",
    "            print(f\"  RMSE: Not available due to insufficient data\")\n",
    "            print(f\"  MAE: Not available due to insufficient data\")\n",
    "\n",
    "    # Optionally print top 10 feature importances\n",
    "    for level in levels:\n",
    "        if importances:\n",
    "            feature_names = data.columns[2:]\n",
    "            importances_data = sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top 10 Feature Importances:\")\n",
    "            for name, importance in importances_data[:10]:\n",
    "                print(f\"{name}: {importance:.4f}\")\n",
    "            return importances_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64080fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_data_2 = rand_forest_levels(undersampled_data_2_top_features, importances=True)\n",
    "importance_data_5 = rand_forest_levels(undersampled_data_5_top_features, importances=True)\n",
    "importance_data_10 = rand_forest_levels(undersampled_data_10_top_features, importances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db3f28",
   "metadata": {},
   "source": [
    "### PLS levels comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fe4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls_levels(data, k=10, graph=False, n_components=9):\n",
    "    X = data[data.columns[2:]]  # Select features starting from 3rd column onwards.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=4)\n",
    "    \n",
    "    # Initialise dictionaries to store metrics and errors\n",
    "    r2_scores = {}\n",
    "    rmse_scores = {}\n",
    "    mae_scores = {}\n",
    "    \n",
    "    column = 'Concentration'\n",
    "    y = data[column]\n",
    "    levels = sorted(y.unique())\n",
    "    \n",
    "    # Initialise metrics tracking\n",
    "    r2_scores[column] = []\n",
    "    rmse_scores[column] = []\n",
    "    mae_scores[column] = []\n",
    "    \n",
    "    # Store predictions for final evaluation\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Standardise features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit PLS model\n",
    "        model = PLSRegression(n_components=n_components)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test_scaled).ravel()  # Flatten output for 1D target variable\n",
    "        \n",
    "        # Metrics for this fold\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_scores[column].append(r2)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rmse_scores[column].append(rmse)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores[column].append(mae)\n",
    "        \n",
    "        # Accumulate predictions\n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "    \n",
    "    # Plot last fold results if graph is True\n",
    "    if graph:\n",
    "        plot_observed_vs_predicted(y_test, y_pred, column)\n",
    "    \n",
    "    # Compute and display average metrics\n",
    "    r2_scores[column] = sum(r2_scores[column]) / len(r2_scores[column])\n",
    "    rmse_scores[column] = sum(rmse_scores[column]) / len(rmse_scores[column])\n",
    "    mae_scores[column] = sum(mae_scores[column]) / len(mae_scores[column])\n",
    "    \n",
    "    level_error_stats = evaluate_levels(np.array(all_y_true), np.array(all_y_pred), levels)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Average Evaluation Metrics:\")\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"R2 Score: {r2_scores[column]:.4f}\")\n",
    "    print(f\"RMSE: {rmse_scores[column]:.4f}\")\n",
    "    print(f\"MAE: {mae_scores[column]:.4f}\")\n",
    "    \n",
    "    print(\"\\nErrors by Concentration Level:\")\n",
    "    for level in levels:\n",
    "        print(f\"\\nConcentration Level {level}:\")\n",
    "        print(f\"  Mean Absolute Error: {level_error_stats[level]['mae']:.4f}\")\n",
    "        print(f\"  RMSE: {level_error_stats[level]['rmse']:.4f}\")\n",
    "    \n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_levels(data_2, n_components= 14)\n",
    "pls_levels(data_5, n_components= 9)\n",
    "pls_levels(data_10, n_components= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806da4c9",
   "metadata": {},
   "source": [
    "## Identifying ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weighted_center_ranges(wavelength_importances, max_gap):\n",
    "    # Convert wavelengths to float and sort the list by wavelength values\n",
    "    wavelength_importances = [(float(wl), imp) for wl, imp in wavelength_importances]\n",
    "    wavelength_importances.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Initialise variables to track ranges and calculations\n",
    "    ranges = []\n",
    "    # Start of the current range\n",
    "    start = wavelength_importances[0][0]\n",
    "    # Weighted sum of wavelengths\n",
    "    weighted_sum = wavelength_importances[0][1] * wavelength_importances[0][0]\n",
    "    # Sum of importance values\n",
    "    total_importance = wavelength_importances[0][1]\n",
    "    # Counter for wavelengths in the current range\n",
    "    count = 1\n",
    "    # Previous wavelength\n",
    "    prev = start\n",
    "\n",
    "    # Iterate through sorted wavelength-importance pairs\n",
    "    for wl, imp in wavelength_importances[1:]:\n",
    "        # If the current wavelength exceeds the maximum allowed gap from the previous, finalise the current range\n",
    "        if wl > prev + max_gap:\n",
    "            # Calculate the weighted center for the current range\n",
    "            center = weighted_sum / total_importance\n",
    "            # Append the range to the list\n",
    "            ranges.append((start, prev, center))\n",
    "            # Start a new range\n",
    "            start = wl\n",
    "            # Reset the weighted sum for the new range\n",
    "            weighted_sum = wl * imp\n",
    "            # Reset the total importance for the new range\n",
    "            total_importance = imp\n",
    "            # Reset count for the new range\n",
    "            count = 1\n",
    "        else:\n",
    "            # If within the gap, add to the current range\n",
    "            # Update the weighted sum\n",
    "            weighted_sum += wl * imp\n",
    "            # Update the total importance\n",
    "            total_importance += imp\n",
    "            # Increment count\n",
    "            count += 1\n",
    "        # Update previous wavelength\n",
    "        prev = wl\n",
    "\n",
    "    # After the loop, add the last range\n",
    "    # Calculate the final center\n",
    "    center = weighted_sum / total_importance\n",
    "    # Append the final range\n",
    "    ranges.append((start, prev, center))\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"Path Length 2\": importance_data_2,\n",
    "    \"Path Length 5\": importance_data_5,\n",
    "    \"Path Length 10\": importance_data_10\n",
    "}\n",
    "wavelength_columns = [col for col in data.columns[2::]]\n",
    "\n",
    "# Iterate through each dataset in the dictionary\n",
    "for name,df in dataframes.items():\n",
    "    print('Dataset: ' + name)\n",
    "    # List to store selected columns based on their importance\n",
    "    selected_column_indices = []\n",
    "    # Define the maximum gap between wavelengths for grouping\n",
    "    ran = 15\n",
    "    \n",
    "    # Find weighted center ranges for the current dataset with the specified maximum gap\n",
    "    ranges = find_weighted_center_ranges(df, ran)\n",
    "    for start, end, center in ranges:\n",
    "        print(f\"Range: {start}-{end} nm, Center: {center:.2f} nm\")\n",
    "        # Find and store the columns within the identified range\n",
    "        selected_column_indices.extend(\n",
    "        [col for col in wavelength_columns if float(col) >= start and float(col) <= end]\n",
    "        )\n",
    "    print(selected_column_indices)\n",
    "    print(len(selected_column_indices))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_2 = start_columns + [1391.64, 1394.97, 1398.31, 1401.68, 1405.05, 1408.45, 1411.86, 2135.89, 2143.74, 2151.65, 2159.62, 2167.65, 2175.73]\n",
    "selected_columns_5 = start_columns + [1278.72, 1281.53, 1284.35, 1287.19, 1290.04, 1292.9, 1295.77, 1325.22, 1328.24, 1331.27, 1334.32, 1337.38, 1340.45, 1343.54, 1346.64, 1349.76, 1352.89, 1356.04, 1359.2, 1362.38, 1365.57, 1368.77, 1371.99, 1695.05]\n",
    "selected_columns_10 = start_columns + [1121.34, 1340.45, 1343.54, 1346.64, 1362.38, 1502.83, 1506.71, 1510.61, 1514.54, 1518.48, 1522.45, 1526.43, 1530.44, 1534.47, 1538.51, 1542.58, 1546.68, 1550.79, 1554.92, 1559.08, 1563.26, 1567.46, 1571.69, 1575.94, 1580.21, 1584.5, 1588.82, 1593.16, 1597.53, 1601.91, 1606.33, 1610.76, 1615.23, 1619.71]\n",
    "\n",
    "general_features_2 = undersampled_data_2[selected_columns_2]\n",
    "general_features_5 = undersampled_data_5[selected_columns_5]\n",
    "general_features_10 = undersampled_data_10[selected_columns_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"Path Length 2\": general_features_2,\n",
    "    \"Path Length 5\": general_features_5,\n",
    "    \"Path Length 10\": general_features_10\n",
    "}\n",
    "\n",
    "for name,df in dataframes.items():\n",
    "    print('Dataset: ' + name)\n",
    "    rand_forest(df)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7dfb26",
   "metadata": {},
   "source": [
    "#### Using the levels on these general ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030103fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"Path Length 2\": general_features_2,\n",
    "    \"Path Length 5\": general_features_5,\n",
    "    \"Path Length 10\": general_features_10\n",
    "}\n",
    "\n",
    "for name,df in dataframes.items():\n",
    "    print('Dataset: ' + name)\n",
    "    rand_forest_levels(df)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108ce9a",
   "metadata": {},
   "source": [
    "## Shap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddeca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_shap_analysis(df, model_params=None, random_state=4):\n",
    "    if model_params is None:\n",
    "        model_params = {'random_state': random_state}\n",
    "\n",
    "    X = df.iloc[:, 2:]\n",
    "    y = df['Concentration']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    model = RandomForestRegressor(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Initialise SHAP explainer object on the trained model and training data\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "    # Ensure column names are strings for SHAP\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    # Selecting a specific test example to display SHAP force plot\n",
    "    index = 3\n",
    "    predicted_value = model.predict(X_test.iloc[index:index+1, :])[0]\n",
    "    true_value = y_test.iloc[index]\n",
    "\n",
    "    # Initialise JavaScript visualisation in Jupyter notebook\n",
    "    shap.initjs()\n",
    "    # Create a summary dot plot showing the impact of each feature on model output\n",
    "    shap.summary_plot(shap_values.values, X_test, plot_type=\"dot\")\n",
    "    \n",
    "    # Generate SHAP stacked force plot for a specific prediction\n",
    "    force_plot = shap.force_plot(explainer.expected_value, shap_values.values, X_test)\n",
    "    \n",
    "    # Create a waterfall plot for detailed decomposition of a specific prediction\n",
    "    shap.waterfall_plot(shap_values[index])\n",
    "\n",
    "    # Display the force plot\n",
    "    display(force_plot)\n",
    "\n",
    "\n",
    "    print(f\"Predicted y-value for the selected test instance: {predicted_value}\")\n",
    "    print(f\"True y-value for the selected test instance: {true_value}\")\n",
    "\n",
    "    return model, explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    #\"Path Length 2\":undersampled_data_2_top_features,\n",
    "    #\"Path Length 5\":undersampled_data_5_top_features,\n",
    "    \"Path Length 10\":undersampled_data_10_top_features\n",
    "}\n",
    "\n",
    "for df_name, df in datasets.items():\n",
    "    print(f\"Performing SHAP analysis for: {df_name}\")\n",
    "    perform_shap_analysis(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
